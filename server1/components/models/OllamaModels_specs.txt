
ollama.generate( pRequest )
  pRequest       <Object>:                     The request object containing generate parameters.
   {  model      <string>:                     The name of the model to use for the chat.
   ,  prompt     <string>:                     The prompt to send to the model.
   ,  suffix     <string>:          (Optional) Suffix is the text that comes after the inserted text.
   ,  system     <string>:          (Optional) Override the model system prompt.
   ,  template   <string>:          (Optional) Override the model template.
   ,  raw        <boolean>:         (Optional) Bypass the prompt template and pass the prompt directly to the model.
   ,  images     <Uint8Array[] | string[]>: (Optional) Images to be included, either as Uint8Array or base64 encoded strings.
   ,  format     <string>:          (Optional) Set the expected format of the response (json).
   ,  stream     <boolean>:         (Optional) When true an AsyncGenerator is returned.
   ,  think      <boolean>:         (Optional) When true, the model will think about the response before responding. Requires thinking support from the model.
   ,  keep_alive <string | number>: (Optional) How long to keep the model loaded. A number (seconds) or a string with a duration unit suffix ("300ms", "1.5h", "2h45m", etc.)
   ,  options    <Options>:         (Optional) Options to configure the runtime.
       {  temperature (float)     :            Controls randomness (0.0–2.0). Lower values make output more deterministic; higher values increase creativity.
       ,  top_p       (float)     :            Limits token sampling to the smallest set with cumulative probability above `top_p` (0.0–1.0).
       ,  top_k       (integer)   :            Limits token sampling to the top `k` most likely tokens.
       ,  num_ctx     (integer)   :            Sets the context window size (number of tokens), e.g. 2048
       ,  seed`       (integer)   :            Sets a random seed for reproducible outputs. e.g. 42
          }
      }


ollama.chat( pRequest )

  pRequest       <Object>:                     The request object containing chat parameters.
   {  model      <string>:                     The name of the model to use for the chat.
   ,  messages   <Message[]>:                  Array of message objects representing the chat history.
   ,  role       <string>:                     The role of the message sender ('user', 'system', or 'assistant').
   ,  content    <string>:                     The content of the message.
   ,  images     <Uint8Array[] | string[]>: (Optional) Images to be included in the message, either as Uint8Array or base64 encoded strings.
   ,  format     <string>:          (Optional) Set the expected format of the response (json).
   ,  stream     <boolean>:         (Optional) When true an AsyncGenerator is returned.
   ,  think      <boolean>:         (Optional) When true, the model will think about the response before responding. Requires thinking support from the model.
   ,  keep_alive <string | number>: (Optional) How long to keep the model loaded. A number (seconds) or a string with a duration unit suffix ("300ms", "1.5h", "2h45m", etc.)
   ,  tools      <Tool[]>:          (Optional) A list of tool calls the model may make.
   ,  options    <Options>:         (Optional) Options to configure the runtime.
      }
 pResponse  
   ,   <ChatResponse>      